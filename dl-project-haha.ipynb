{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11041120,"sourceType":"datasetVersion","datasetId":6877483}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import LongformerModel, LongformerTokenizer, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.735270Z","iopub.execute_input":"2025-03-15T15:59:43.735545Z","iopub.status.idle":"2025-03-15T15:59:43.740945Z","shell.execute_reply.started":"2025-03-15T15:59:43.735517Z","shell.execute_reply":"2025-03-15T15:59:43.740063Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/for-model-1/merged_df (1).csv')\ndf = df[['text', 'multi_labels','hier_label']]\n# df['multi_labels'] = df['multi_labels'].apply(ast.literal_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.741922Z","iopub.execute_input":"2025-03-15T15:59:43.742236Z","iopub.status.idle":"2025-03-15T15:59:43.770397Z","shell.execute_reply.started":"2025-03-15T15:59:43.742209Z","shell.execute_reply":"2025-03-15T15:59:43.769405Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def list_ebal(df):\n    # Преобразуем строки меток в списки (если они ещё не списки)\n    df['multi_labels'] = df['multi_labels'].apply(eval)  # eval используется для преобразования строки в список\n    # df['hier_label'] = df['hier_label'].apply(eval)  # аналогично для иерархических меток\n    return df\n\ndf = list_ebal(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.771431Z","iopub.execute_input":"2025-03-15T15:59:43.771767Z","iopub.status.idle":"2025-03-15T15:59:43.778584Z","shell.execute_reply.started":"2025-03-15T15:59:43.771738Z","shell.execute_reply":"2025-03-15T15:59:43.777726Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df.head(\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.779615Z","iopub.execute_input":"2025-03-15T15:59:43.779880Z","iopub.status.idle":"2025-03-15T15:59:43.807649Z","shell.execute_reply.started":"2025-03-15T15:59:43.779844Z","shell.execute_reply":"2025-03-15T15:59:43.806777Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  Трамп вновь пригрозил последствиями Ирану, есл...   \n1  Ким Чен Ын пообещал скоро представить новое ст...   \n2  В России почти 6,5 миллиона человек встретили ...   \n3  Столкновения произошли в Гонконге во время сог...   \n4  Папа Римский Франциск извинился за то, что нак...   \n\n                                        multi_labels  \\\n0  [Международные отношения, Политика, Происшествия]   \n1    [Международные отношения, Политика, Вооружение]   \n2         [Общество, Региональные новости, Политика]   \n3  [Международные отношения, Происшествия, Политика]   \n4       [Международные отношения, Религия, Общество]   \n\n                                 hier_label  \n0  ['Международные отношения', 'Конфликты']  \n1   ['Международные отношения', 'Политика']  \n2                 ['Общество', 'Праздники']  \n3   ['Международные отношения', 'Протесты']  \n4    ['Международные отношения', 'Религия']  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>multi_labels</th>\n      <th>hier_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Трамп вновь пригрозил последствиями Ирану, есл...</td>\n      <td>[Международные отношения, Политика, Происшествия]</td>\n      <td>['Международные отношения', 'Конфликты']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ким Чен Ын пообещал скоро представить новое ст...</td>\n      <td>[Международные отношения, Политика, Вооружение]</td>\n      <td>['Международные отношения', 'Политика']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>В России почти 6,5 миллиона человек встретили ...</td>\n      <td>[Общество, Региональные новости, Политика]</td>\n      <td>['Общество', 'Праздники']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Столкновения произошли в Гонконге во время сог...</td>\n      <td>[Международные отношения, Происшествия, Политика]</td>\n      <td>['Международные отношения', 'Протесты']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Папа Римский Франциск извинился за то, что нак...</td>\n      <td>[Международные отношения, Религия, Общество]</td>\n      <td>['Международные отношения', 'Религия']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# ================================\n# 1. Определяем модель\n# ================================\n\nclass HierarchicalMultiTaskLongformer(nn.Module):\n    def __init__(self, model_name, num_multi_labels, num_hier_labels):\n        \"\"\"\n        :param model_name: имя предобученной модели, например, \"allenai/longformer-base-4096\"\n        :param num_multi_labels: число классов для мультилейблинга\n        :param num_hier_labels: число классов для иерархической классификации\n        \"\"\"\n        super(HierarchicalMultiTaskLongformer, self).__init__()\n        self.longformer = LongformerModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.1)\n        # Голова для мультилейблинга\n        self.multi_label_classifier = nn.Linear(self.longformer.config.hidden_size, num_multi_labels)\n        # Голова для иерархической классификации\n        self.hierarchical_classifier = nn.Linear(self.longformer.config.hidden_size, num_hier_labels)\n        # Функции потерь\n        self.multi_label_loss_fn = nn.BCEWithLogitsLoss()\n        self.hierarchical_loss_fn = nn.CrossEntropyLoss()\n    \n    def forward(self, input_ids, attention_mask, multi_labels=None, hier_labels=None):\n        outputs = self.longformer(input_ids, attention_mask=attention_mask)\n        # Используем представление первого токена ([CLS])\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        ml_logits = self.multi_label_classifier(cls_output)\n        hier_logits = self.hierarchical_classifier(cls_output)\n        \n        loss = None\n        if multi_labels is not None and hier_labels is not None:\n            ml_loss = self.multi_label_loss_fn(ml_logits, multi_labels)\n            hier_loss = self.hierarchical_loss_fn(hier_logits, hier_labels)\n            loss = ml_loss + hier_loss  # Можно добавить веса для балансировки\n        return ml_logits, hier_logits, loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.808481Z","iopub.execute_input":"2025-03-15T15:59:43.808763Z","iopub.status.idle":"2025-03-15T15:59:43.823422Z","shell.execute_reply.started":"2025-03-15T15:59:43.808730Z","shell.execute_reply":"2025-03-15T15:59:43.822453Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ================================\n# 2. Определяем Dataset для новостных данных\n# ================================\n\nclass NewsDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, mlb, hier_label_map):\n        \"\"\"\n        :param dataframe: DataFrame с колонками 'text', 'multi_labels', 'hier_label'\n        :param tokenizer: токенизатор модели\n        :param max_length: максимальная длина последовательности\n        :param mlb: объект MultiLabelBinarizer для мультилейблинга\n        :param hier_label_map: словарь для маппинга иерархических меток в числа\n        \"\"\"\n        self.df = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.mlb = mlb\n        self.hier_label_map = hier_label_map\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        text = row['text']\n        # Токенизация текста\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        input_ids = encoding['input_ids'].squeeze()          # [max_length]\n        attention_mask = encoding['attention_mask'].squeeze()  # [max_length]\n        \n        # Преобразуем мультиметочные метки в бинарный вектор\n        multi_labels = self.mlb.transform([row['multi_labels']])[0]\n        multi_labels = torch.tensor(multi_labels, dtype=torch.float)\n        \n        # Преобразуем иерархическую метку в числовое значение\n        hier_label_str = row['hier_label']\n        hier_label = self.hier_label_map[hier_label_str]\n        hier_label = torch.tensor(hier_label, dtype=torch.long)\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'multi_labels': multi_labels,\n            'hier_labels': hier_label\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.824434Z","iopub.execute_input":"2025-03-15T15:59:43.824677Z","iopub.status.idle":"2025-03-15T15:59:43.842884Z","shell.execute_reply.started":"2025-03-15T15:59:43.824647Z","shell.execute_reply":"2025-03-15T15:59:43.842197Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ================================\n# 3. Загрузка данных и подготовка разметки\n# ================================\ndef create_hier_label_map(df):\n    \"\"\"Создаём маппинг для иерархических меток.\"\"\"\n    unique_hier_labels = df['hier_label'].explode().unique()  # Разворачиваем списки и находим уникальные метки\n    hier_label_map = {label: idx for idx, label in enumerate(unique_hier_labels)}\n    return hier_label_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.843700Z","iopub.execute_input":"2025-03-15T15:59:43.843916Z","iopub.status.idle":"2025-03-15T15:59:43.862485Z","shell.execute_reply.started":"2025-03-15T15:59:43.843897Z","shell.execute_reply":"2025-03-15T15:59:43.861788Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ================================\n# 4. Тренировочный цикл\n# ================================\n\ndef train_model(model, dataloader, optimizer, scheduler, device, epochs):\n    model.train()\n    for epoch in tqdm(range(epochs)):\n        epoch_loss = 0.0\n        for batch in dataloader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            multi_labels = batch['multi_labels'].to(device)\n            hier_labels = batch['hier_labels'].to(device)\n            \n            ml_logits, hier_logits, loss = model(input_ids, attention_mask, multi_labels, hier_labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            epoch_loss += loss.item()\n        avg_loss = epoch_loss / len(dataloader)\n        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.863393Z","iopub.execute_input":"2025-03-15T15:59:43.863685Z","iopub.status.idle":"2025-03-15T15:59:43.879038Z","shell.execute_reply.started":"2025-03-15T15:59:43.863655Z","shell.execute_reply":"2025-03-15T15:59:43.878118Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Параметры обучения\nmodel_name = \"allenai/longformer-base-4096\"\nmax_length = 512\nbatch_size = 4\nepochs = 10\nlearning_rate = 2e-5\n\n# Создаём маппинг для иерархических меток\nhier_label_map = create_hier_label_map(df)\nnum_hier_labels = len(hier_label_map)\n\n# Определяем фиксированный набор классов для мультилейблинга (знаем заранее)\n# Получаем уникальные элементы\nunique_elements = list(set([item for sublist in df['multi_labels'] for item in sublist]))\nall_multi_labels = unique_elements\nmlb = MultiLabelBinarizer(classes=all_multi_labels)\nmlb.fit(df['multi_labels'])\nnum_multi_labels = len(all_multi_labels)\n\n# Инициализируем токенизатор и модель\ntokenizer = LongformerTokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = HierarchicalMultiTaskLongformer(model_name, num_multi_labels, num_hier_labels)\nmodel.to(device)\n\n# Создаём Dataset и DataLoader\ndataset = NewsDataset(df, tokenizer, max_length, mlb, hier_label_map)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:43.880021Z","iopub.execute_input":"2025-03-15T15:59:43.880312Z","iopub.status.idle":"2025-03-15T15:59:46.105950Z","shell.execute_reply.started":"2025-03-15T15:59:43.880291Z","shell.execute_reply":"2025-03-15T15:59:46.104632Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Оптимизатор и scheduler\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Обучение модели\ntrained_model = train_model(model, dataloader, optimizer, scheduler, device, epochs)\n\n# Сохраняем веса модели\ntorch.save(trained_model.state_dict(), \"fine_tuned_longformer.pt\")\nprint(\"Модель сохранена как fine_tuned_longformer.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:59:49.817646Z","iopub.execute_input":"2025-03-15T15:59:49.817955Z","iopub.status.idle":"2025-03-15T16:01:57.055575Z","shell.execute_reply.started":"2025-03-15T15:59:49.817928Z","shell.execute_reply":"2025-03-15T16:01:57.054665Z"}},"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [00:13<02:02, 13.57s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Average Loss: 3.6785\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [00:26<01:43, 12.97s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Average Loss: 3.2754\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [00:38<01:29, 12.77s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Average Loss: 3.1550\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [00:51<01:16, 12.67s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Average Loss: 3.1111\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [01:03<01:03, 12.62s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Average Loss: 2.8681\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [01:16<00:50, 12.59s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Average Loss: 2.7259\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [01:28<00:37, 12.57s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Average Loss: 2.5389\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [01:41<00:25, 12.56s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Average Loss: 2.5209\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [01:53<00:12, 12.55s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Average Loss: 2.4398\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [02:06<00:00, 12.63s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Average Loss: 2.3252\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Модель сохранена как fine_tuned_longformer.pt\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom transformers import LongformerTokenizer\n\n# Параметры модели (эти параметры должны соответствовать тем, что использовались при обучении)\nmodel_name = \"allenai/longformer-base-4096\"\nnum_multi_labels = num_multi_labels   # например, если у нас 5 меток для мультилейблинга\nnum_hier_labels = num_hier_labels   # например, если у нас 3 класса для иерархической классификации\n\n# Инициализация модели и загрузка сохранённых весов\nmodel = HierarchicalMultiTaskLongformer(model_name, num_multi_labels, num_hier_labels)\nmodel.load_state_dict(torch.load(\"fine_tuned_longformer.pt\", map_location=torch.device(\"cpu\"), weights_only=False))\nmodel.eval()  # Переводим модель в режим инференса\n\n# Инициализация токенизатора\ntokenizer = LongformerTokenizer.from_pretrained(model_name)\n\n# Пример входного текста\nsample_text = \"Новая экономическая инициатива обсуждается на высшем уровне.\"\n\n# Токенизация текста\ninputs = tokenizer(\n    sample_text,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    truncation=True,\n    max_length=512\n)\n\n# Выполнение инференса (без вычисления градиентов)\nwith torch.no_grad():\n    ml_logits, hier_logits, _ = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"]\n    )\n\n# Преобразование логитов в вероятности\nml_probs = torch.sigmoid(ml_logits)       # Вероятности для мультилейблинга\nhier_probs = torch.softmax(hier_logits, dim=1)  # Вероятности для иерархической классификации\n\nprint(\"Логиты для мультилейблинга:\", ml_logits)\nprint(\"Вероятности для мультилейблинга:\", ml_probs)\nprint(\"Логиты для иерархической классификации:\", hier_logits)\nprint(\"Вероятности для иерархической классификации:\", hier_probs)\n\n# Дополнительно: если требуется получить CLS-токен (вектор эмбеддингов)\n# Его размерность будет [batch_size, hidden_size] (например, [1, 768] для base модели)\n# Обычно CLS-токен используется внутри модели для классификации, но можно его извлечь так:\nwith torch.no_grad():\n    outputs = model.longformer(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"]\n    )\ncls_token_output = outputs.last_hidden_state[:, 0, :]  # Вектор CLS токена\nprint(\"Вектор CLS токена:\", cls_token_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:02:43.722664Z","iopub.execute_input":"2025-03-15T16:02:43.722963Z","iopub.status.idle":"2025-03-15T16:02:47.598574Z","shell.execute_reply.started":"2025-03-15T16:02:43.722936Z","shell.execute_reply":"2025-03-15T16:02:47.597502Z"}},"outputs":[{"name":"stdout","text":"Логиты для мультилейблинга: tensor([[-1.9676, -1.6637, -0.8333,  0.7046, -1.5735, -2.0407, -2.4252, -1.4855,\n         -1.5834, -1.2678, -1.7714, -0.8604, -0.4174, -2.2043, -2.3345, -1.4021,\n         -1.9903,  0.9811, -1.6408, -0.7815]])\nВероятности для мультилейблинга: tensor([[0.1226, 0.1593, 0.3029, 0.6692, 0.1717, 0.1150, 0.0813, 0.1846, 0.1703,\n         0.2196, 0.1454, 0.2973, 0.3971, 0.0994, 0.0883, 0.1975, 0.1202, 0.7273,\n         0.1624, 0.3140]])\nЛогиты для иерархической классификации: tensor([[ 0.6467,  1.1650, -1.0914, -0.5558, -0.6637, -0.1290,  0.3603,  1.2253,\n         -0.7078, -0.2891,  0.0159, -0.9071,  1.2728,  1.9451,  1.1688, -0.5179,\n         -0.1692,  0.3464, -0.6740,  0.0645, -1.2849, -0.4379, -0.2997, -1.4251,\n         -0.3742, -0.9130, -1.1035, -0.7495, -0.0920,  0.0205,  0.2829, -0.2161]])\nВероятности для иерархической классификации: tensor([[0.0466, 0.0782, 0.0082, 0.0140, 0.0126, 0.0214, 0.0350, 0.0831, 0.0120,\n         0.0183, 0.0248, 0.0098, 0.0871, 0.1706, 0.0785, 0.0145, 0.0206, 0.0345,\n         0.0124, 0.0260, 0.0067, 0.0157, 0.0181, 0.0059, 0.0168, 0.0098, 0.0081,\n         0.0115, 0.0222, 0.0249, 0.0324, 0.0197]])\nВектор CLS токена: tensor([[ 1.0425e+00,  8.2453e-01,  1.3049e+00, -3.7406e-02,  1.9021e+00,\n         -6.8522e-01, -1.2805e-01, -4.1075e-02,  8.1305e-01, -1.7439e-01,\n         -1.4046e+00,  4.4496e-01, -3.8540e-02,  1.2207e+00,  6.9384e-01,\n         -1.8309e+00, -7.2392e-01,  5.6431e-02,  6.0082e-02,  3.4973e-01,\n         -6.6015e-01,  8.3611e-01,  2.0739e-01,  4.7946e-01, -3.5588e-01,\n         -6.1527e-01,  1.4435e-02, -5.7746e-01, -1.5302e-01,  1.9029e-01,\n          1.0451e+00, -7.7469e-02,  1.5805e-01, -2.8929e-01,  1.1523e+00,\n          1.1373e+00, -3.4463e-01, -4.6527e-01, -2.3517e-02, -8.6344e-03,\n          4.1038e-01, -9.1539e-01,  1.0222e+00, -2.3033e-01,  6.9049e-01,\n         -6.1453e-01, -3.8738e-01,  3.2748e-01, -7.7358e-02,  4.1625e-02,\n         -1.8525e-01,  1.4299e+00,  5.2210e-01, -6.5386e-02, -1.6029e-01,\n         -1.0951e-01,  1.3034e-01,  1.1358e+00,  1.0979e+00, -1.5891e-01,\n          6.3380e-01,  3.5199e-01,  8.9970e-01,  7.2061e-01,  1.5786e+00,\n          1.3368e-01,  3.0458e-01, -1.2634e+00,  6.1358e-01, -6.0521e-01,\n         -6.9601e-01, -7.6307e-01,  6.6716e-01, -8.1844e-01, -9.3331e-03,\n          1.2431e+00, -8.2226e-01, -2.8547e+00, -1.7278e+00, -1.2619e-01,\n         -5.3915e-01,  7.2241e-01,  7.5511e-01,  3.2648e-01, -9.4312e-01,\n         -7.5861e-01,  2.0406e-03,  2.8700e-01,  1.6472e+00,  1.0871e+00,\n         -1.5592e-01,  3.7056e-01,  6.5289e-01, -3.0630e-01,  9.7200e-01,\n         -4.6790e-02,  3.8893e-01,  2.3768e-02,  3.1365e-01, -1.6648e+00,\n         -1.2736e+00, -4.0592e-01,  1.6330e-03,  2.4056e-01, -1.6601e-02,\n         -1.7439e-01, -2.2141e+00,  3.7900e-02, -8.0030e-01,  5.3667e-02,\n         -1.2375e+00, -4.3885e-01, -2.3018e-01,  1.3151e-01, -8.9538e-01,\n         -1.2133e-02, -3.3885e-01, -1.1002e+00,  6.9787e-01, -1.5618e+00,\n         -1.2055e+00,  5.1551e-01,  1.1922e+00, -5.8716e-01, -1.1701e+00,\n         -1.1922e+00,  6.1500e-01, -2.6324e-01, -4.5577e-01, -1.7736e+00,\n         -1.6594e-01,  1.5320e-02,  7.7382e-01, -2.7995e-01,  2.4175e-01,\n         -3.9638e-02,  1.8913e-01, -4.5471e-03,  1.2700e-01, -5.5475e-01,\n          7.1347e-02, -1.0820e-01, -2.6239e-01,  1.4762e-01, -6.4293e-01,\n         -5.7266e-01, -1.6144e+00,  1.0507e+00, -4.7995e-01,  3.6578e-01,\n          5.2559e-01, -2.9420e-01,  4.9771e-01,  4.3885e-02, -2.6874e-01,\n         -2.7746e-01, -1.6474e-01,  7.7952e-02, -3.8366e-01, -4.8501e-01,\n         -3.0083e-02, -1.3861e+00,  5.9606e-01,  1.4891e-01,  7.9327e-01,\n          8.4259e-01,  1.7581e-01,  1.1344e+00,  1.8267e-01,  1.0671e+00,\n          1.6601e-02,  1.2830e+00,  8.8217e-02,  7.0470e-01, -6.8682e-01,\n         -6.1952e-01,  1.2039e-01, -4.3845e-01, -1.1724e+00, -1.0950e+00,\n         -4.4789e-01,  7.7381e-01, -3.3405e-01, -2.2261e-01,  8.1212e-02,\n         -1.7213e-01, -1.0323e+00,  9.7634e-02, -2.6954e-01,  5.4224e-01,\n          5.5772e-01, -1.1242e+00,  9.6274e-01,  5.6540e-01, -5.3553e-01,\n         -4.2478e-01,  8.7147e-01, -3.5957e-01, -2.2574e-02, -5.8948e-01,\n         -1.2033e-01,  1.9080e+00, -2.2263e-01, -6.4569e-01,  1.3053e+00,\n         -5.5034e-01,  1.1492e+00,  1.2163e-01, -6.6027e-02,  1.3515e-02,\n          6.3074e-01,  3.9587e-01,  7.9449e-01, -5.6647e-01,  1.2003e-01,\n         -7.3429e-01,  7.2613e-01, -4.1956e-01,  1.2269e+00,  3.5975e-01,\n         -7.6476e-02,  5.9571e-01, -9.3447e-01, -8.1097e-01,  2.0868e+00,\n          1.1780e+00, -3.7725e-01,  1.6801e+00,  3.9065e-02,  3.3596e-01,\n         -5.6336e-01, -8.7624e-01, -1.2844e+00,  5.1378e-03,  6.5909e-01,\n          1.9505e+00, -7.2458e-01, -4.6716e-02, -7.1553e-01,  2.8536e-01,\n          5.4432e-01,  3.1644e-01,  9.0828e-01,  4.2638e-01,  7.7698e-01,\n          1.0898e+00,  2.0553e-01,  6.0515e-01,  1.1927e+00,  1.5563e-01,\n          5.4934e-01, -8.6662e-01, -1.4102e+00, -8.1329e-01,  3.1717e-01,\n          9.5440e-01,  7.8830e-01, -8.1770e-01, -9.8973e-01,  1.8543e+00,\n          1.2612e+00, -5.8105e-01,  7.1149e-01,  3.0513e-01,  1.1253e+00,\n          1.4991e-01, -3.7167e-02,  6.8383e-01, -1.0940e-02,  1.8168e-01,\n          4.0731e-01,  5.7614e-01, -1.8610e-01,  1.7946e-01,  8.8914e-01,\n          4.8461e-01,  9.7102e-01, -1.1042e+00,  6.4717e-01,  7.5580e-01,\n          3.0477e-01,  1.5647e-01, -1.3398e+00,  1.8429e+00,  4.2329e-01,\n         -2.2885e-01,  1.1377e+00,  3.8354e-02, -2.1194e-01, -1.0892e-02,\n         -2.0541e-02, -1.9100e-01,  4.1871e-01, -9.1104e-02, -4.5236e-01,\n          3.9477e-01, -2.7515e-01,  2.2568e-01,  2.9242e-01, -3.4053e-01,\n         -1.6780e+00, -2.2438e+00,  3.0870e-01,  1.4817e-01,  5.8173e-01,\n         -1.7072e-01, -1.7137e-01, -9.0405e-01,  1.3794e+00, -1.5858e-01,\n         -1.0536e-01, -7.8228e-01, -7.5163e-02,  1.9568e-01,  5.0564e-01,\n          8.0961e-02,  3.4108e-01, -1.3466e+00, -6.0702e-01, -6.3723e-01,\n          3.1146e-01,  2.4863e-02,  3.0916e-01,  7.5436e-01, -1.3722e+00,\n         -8.5283e-01, -1.1075e+00, -2.7597e-01, -9.5724e-01,  1.7205e-01,\n         -1.1374e-01,  2.4671e+00, -1.1388e-01,  2.2093e+00,  7.9144e-02,\n          7.5274e-01, -7.3463e-02, -4.9836e-01,  1.5441e-01, -6.8291e-01,\n         -5.2637e-02, -6.9388e-01,  7.8871e-01, -3.7308e-01, -2.2551e-01,\n          4.6132e-01,  3.9805e-02,  1.4288e-01, -8.4898e-01, -4.2355e-01,\n         -3.3085e-01,  4.8564e-02,  4.5180e-01, -4.9950e-02, -2.3413e-01,\n          1.3276e-01,  7.9651e-01,  5.7216e-01,  4.0283e-01, -6.0789e-01,\n          3.9492e-01, -4.1004e-01,  1.4458e+00, -3.1090e-01, -5.9737e-01,\n         -5.5534e-01, -4.4399e-01,  3.4320e-01,  8.9496e-01,  1.1191e+00,\n         -1.3258e-01, -1.0233e+00, -4.3458e-01, -3.3705e-01, -4.4197e-02,\n          1.0385e-01,  9.5622e-01, -1.0195e+00, -1.4650e-01, -1.4418e-01,\n          3.9304e-01, -1.5461e+00, -6.6373e-02, -3.9900e-02,  5.7193e-02,\n         -1.4338e+00, -6.1925e-01, -1.0836e+00,  7.2587e-02, -1.3991e+00,\n          1.5375e+00, -2.7322e-01,  3.2848e-01, -4.0666e-01, -7.4559e-02,\n          7.3017e-01,  7.9293e-01,  1.3257e+00,  6.8029e-01,  1.0219e-01,\n          2.8561e-01,  1.2516e-01, -8.5150e-01, -2.4401e+00, -2.4601e-01,\n         -3.6140e-01, -1.4852e+00, -9.3799e-01, -1.4026e-01, -7.5447e-01,\n         -2.6791e-01, -2.2257e-01,  1.2429e-01,  8.4550e-02,  3.7790e-01,\n         -9.4460e-01, -2.9287e-01, -4.2853e-01,  7.8666e-02, -1.0746e+00,\n          1.4041e-01,  2.2973e-01, -4.0952e-01,  2.0670e-01, -4.2482e-01,\n          8.6181e-01,  8.0135e-01, -1.0287e+00, -7.9325e-01,  6.9585e-01,\n          1.2154e+00,  4.9584e-01,  7.7842e-01,  4.9565e-01, -6.4626e-01,\n         -1.6124e+00, -4.7761e-01,  2.2311e-01, -2.7276e-01, -1.0114e+00,\n         -2.3801e-01,  7.4276e-01,  5.7191e-01, -4.5591e-01,  2.5772e-01,\n         -3.1562e-01,  2.5663e-03,  4.4628e-02,  1.6196e+00, -4.4302e-01,\n          4.3306e-01,  3.6946e-02, -2.0560e-01, -2.8223e+00,  8.7376e-01,\n         -1.3802e-01, -7.6994e-01,  1.6614e-01,  6.4757e-02, -2.3767e-01,\n         -2.9899e-01,  1.1928e+00,  2.7644e-01, -2.1170e-01,  6.3018e-01,\n          4.6091e-02, -5.4596e-01,  4.7192e-02,  3.7019e-01, -1.9637e-01,\n         -1.1020e-01, -4.3951e-02,  9.2357e-03,  1.6785e-01,  9.2117e-02,\n          9.8873e-01, -1.2849e+00,  1.6261e+00, -1.1307e+00, -1.3156e+00,\n         -4.9545e-01,  1.0100e+00,  2.2872e-01,  2.1705e-01,  3.0961e-01,\n          6.0618e-02, -5.0422e-01,  8.5658e-01,  9.3462e-01, -1.5082e-01,\n         -6.9130e-01, -2.2631e-01, -5.9077e-01,  7.3247e-01, -5.4448e-01,\n         -2.7311e-01, -9.1437e-01, -7.4248e-01,  2.6093e+00, -9.1714e-02,\n          1.1713e-01, -9.1477e-01,  2.5770e-01, -8.4385e-01,  5.3786e-01,\n         -1.5917e+00,  6.5965e-01, -2.2405e-01,  2.2283e-01, -2.2050e-01,\n          5.6478e-01,  1.0991e+00, -4.5801e-01,  8.3907e-01,  3.0670e-02,\n          3.2460e-01,  9.6653e-01,  5.8333e-02, -1.7803e+00,  1.5145e+00,\n          3.8226e-01, -5.0822e-01, -1.9795e+00, -6.1282e-02, -4.5795e-01,\n          4.0081e-01,  3.2448e-01, -4.8272e-01,  4.0145e-01, -1.5121e+00,\n         -7.0089e-01, -9.0999e-01,  3.6734e-01,  8.9586e-01, -2.4447e-01,\n         -2.0507e-01,  5.1104e-01, -2.1895e-01, -3.3990e-01,  2.7162e+00,\n          7.4342e-01, -8.9201e-02, -8.1965e-01,  1.0218e-01,  1.8116e+00,\n          2.0233e-01,  8.5458e-01,  1.0287e+00, -4.1152e-01,  2.1021e-01,\n          1.5972e+00, -1.8317e+00, -3.8067e-01,  4.8697e-01, -3.9541e-02,\n          4.4359e-01,  6.4479e-01,  7.3155e-01,  8.7367e-01,  1.7790e+00,\n          1.0055e-01,  3.0731e-02,  2.3211e-01,  9.8475e-01,  1.1289e+00,\n         -4.3259e-01,  1.7404e-02,  1.5007e+00,  6.3514e-01,  8.8360e-01,\n          9.0954e-01,  1.0559e+00,  6.9707e-01, -2.3090e-01, -3.9288e-01,\n          6.2036e-01, -7.0191e-01, -6.4719e-01,  3.2068e-01,  1.0817e-01,\n         -4.0893e-01,  3.8581e-01,  2.7438e-01,  6.7626e-01, -3.4643e-01,\n          2.9356e-01, -6.9185e-01, -1.0236e+00,  1.6990e+00,  5.9120e-01,\n          2.7784e-01,  5.1578e-01, -1.2757e-01,  3.1754e-02,  4.9740e-01,\n          6.2110e-01,  5.3400e-01, -8.7347e-01, -4.8636e-02,  5.6250e-01,\n          4.8536e-01, -2.4171e-01, -7.6140e-01, -2.5181e-01,  1.4155e+00,\n          1.8449e+00,  3.1574e-01, -1.0157e-01, -7.2814e-01, -4.5422e-01,\n         -2.4466e-01,  1.9556e+00, -4.3581e-01,  1.7465e-01,  1.0720e+00,\n          1.2532e+00, -5.0827e-02, -8.8646e-01, -1.3054e+00,  1.9204e-01,\n          1.1602e-01,  1.2126e+00, -4.2092e-01,  9.5702e-02, -2.7501e-02,\n          1.3121e+00,  2.7837e-01,  3.7803e-01, -1.7063e-01, -9.6778e-01,\n         -4.1641e-01, -1.4514e-02,  7.6918e-01, -7.0627e-03,  1.7431e-01,\n          1.8857e-01,  1.0802e+00, -7.1743e-02,  7.4869e-01, -8.7228e-01,\n          1.0554e+00,  1.8509e-01, -4.1281e-02, -4.6687e-01, -6.0349e-02,\n         -5.1382e-01,  2.4176e-01,  1.0268e+00, -1.3121e-01,  1.2962e+00,\n          6.0506e-01, -5.5704e-01, -2.0354e-01,  2.9840e-01, -2.6162e-01,\n          3.8385e-01,  8.4469e-01, -1.6120e+00,  8.6707e-01, -2.7605e-01,\n          4.1842e-01, -1.8094e-01,  3.4959e-02, -6.8739e-02,  7.9412e-01,\n         -6.4064e-01,  8.3378e-02, -7.4467e-01, -2.3531e-01,  5.4583e-01,\n          6.8460e-01, -4.4017e-01,  1.9364e+00,  8.9446e-01,  5.7839e-01,\n          5.7368e-01, -9.0460e-01, -4.7311e-01,  1.6315e-01, -1.9754e-01,\n         -1.3468e-01, -4.8966e-01,  1.8869e+00, -3.0175e-01,  3.3683e-01,\n         -1.1837e+00,  1.2196e+00,  9.7879e-01, -1.0973e+00,  2.5169e-01,\n         -6.6190e-01,  6.9726e-02,  5.4355e-01,  4.0082e-01, -3.3350e-01,\n         -1.5561e+00,  4.6887e-01, -3.6290e-01, -8.6291e-01, -1.1129e-02,\n          1.1583e+00,  7.6894e-02, -7.8637e-01,  3.2325e-01, -5.8260e-01,\n         -1.9063e-02,  1.0720e+00,  9.0607e-02,  2.5611e-01, -3.9816e-01,\n         -7.5928e-01, -1.6692e-01,  1.2483e-01,  2.8062e-01,  4.5275e-01,\n          1.0759e+00,  2.4040e-01, -7.7185e-02,  3.1900e-01, -3.3597e-01,\n          1.2552e+00, -3.7760e-01,  3.7550e-01, -5.5725e-01, -6.9397e-01,\n         -3.1566e-01,  1.1852e+00,  7.2554e-01, -4.9735e-01, -5.8283e-01,\n         -9.1646e-02, -1.9024e-01, -1.4505e+00, -1.4713e+00,  9.7661e-01,\n         -1.0297e-01, -6.3928e-01, -6.9838e-01,  3.3703e-02,  4.4275e-01,\n         -1.0507e+00, -1.3259e+00,  6.9691e-01, -5.5308e-01,  1.6201e-01,\n          1.0052e+00,  1.5105e-01,  2.5771e-01,  1.1692e+00, -1.2150e+00,\n          1.2456e+00,  8.6797e-01,  2.3083e-01, -3.5176e-01,  6.9798e-01,\n         -8.3081e-01, -1.4551e+00, -3.3158e-01, -4.7311e-02,  6.7258e-01,\n         -3.8926e-01, -5.1993e-01,  1.0609e+00,  1.8822e-01,  6.0366e-01,\n         -2.2722e+00,  1.1883e+00,  6.6125e-02]])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Пример входного текста\nsample_text = \"Пираты напали на судно  и похитили трех членов экипажа, после чего в перестрелке убили четырех сотрудников спасательной группы ВМС страны, сообщают местные СМИ.\"\n# Токенизация текста\ninputs = tokenizer(\n    sample_text,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    truncation=True,\n    max_length=max_length\n)\n# Переносим тензоры на устройство\ninputs = {k: v.to('cpu') for k, v in inputs.items()}\n\n# Выполняем инференс\nwith torch.no_grad():\n    ml_logits, hier_logits, _ = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"]\n    )\n\n# Обработка результатов для мультилейблинга\nhier_label_names = [label for label, idx in sorted(hier_label_map.items(), key=lambda x: x[1])]\nml_probs = torch.sigmoid(ml_logits)[0]  # [num_multi_labels]\nthreshold = 0.5\npredicted_multi_labels = [all_multi_labels[i] for i, prob in enumerate(ml_probs) if prob > threshold]\n\n# Обработка результатов для иерархической классификации\nhier_probs = torch.softmax(hier_logits, dim=1)[0]  # [num_hier_labels]\npredicted_hier_index = torch.argmax(hier_probs).item()\npredicted_hier_label = hier_label_names[predicted_hier_index]\n\n# Извлекаем вектор CLS токена\nwith torch.no_grad():\n    outputs = model.longformer(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"]\n    )\ncls_token_vector = outputs.last_hidden_state[:, 0, :]  # размер: [batch_size, hidden_size]\n\n# Вывод результатов\nprint(\"Предсказанные метки для мультилейблинга:\")\nprint(predicted_multi_labels)\nprint(\"\\nВероятности для мультилейблинга:\")\nfor label, prob in zip(all_multi_labels, ml_probs.tolist()):\n    print(f\"{label}: {prob:.4f}\")\n\nprint(\"\\nПредсказанная метка для иерархической классификации:\")\nprint(predicted_hier_label)\nprint(\"\\nВероятности для иерархической классификации:\")\nfor label, prob in zip(hier_label_names, hier_probs.tolist()):\n    print(f\"{label}: {prob:.4f}\")\n\nprint(\"\\nВектор CLS токена:\")\nprint(cls_token_vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:02:55.971979Z","iopub.execute_input":"2025-03-15T16:02:55.972343Z","iopub.status.idle":"2025-03-15T16:02:57.513632Z","shell.execute_reply.started":"2025-03-15T16:02:55.972306Z","shell.execute_reply":"2025-03-15T16:02:57.512770Z"}},"outputs":[{"name":"stdout","text":"Предсказанные метки для мультилейблинга:\n['Политика', 'Международные отношения']\n\nВероятности для мультилейблинга:\nЗаконодательство: 0.1395\nЭкономика: 0.1979\nБезопасность: 0.3065\nПолитика: 0.5960\nПреступность: 0.1385\nКино: 0.1566\nЭнергетика: 0.2101\nКультура: 0.1471\nТерроризм: 0.2068\nСпорт: 0.2589\nТуризм: 0.1493\nПроисшествия: 0.3129\nРегиональные новости: 0.1946\nВооружение: 0.1942\nЭкология: 0.2052\nРелигия: 0.2335\nЗдравоохранение: 0.1278\nМеждународные отношения: 0.7009\nТранспорт: 0.1326\nОбщество: 0.2634\n\nПредсказанная метка для иерархической классификации:\n['Международные отношения', 'Политика']\n\nВероятности для иерархической классификации:\n['Международные отношения', 'Конфликты']: 0.0196\n['Международные отношения', 'Политика']: 0.2212\n['Общество', 'Праздники']: 0.0304\n['Международные отношения', 'Протесты']: 0.0129\n['Международные отношения', 'Религия']: 0.0172\n['Транспорт', 'Ж/Д']: 0.0114\n['Экономика', 'Бизнес']: 0.0181\n['Политика', 'Внутренняя политика']: 0.0631\n['Спорт', 'Баскетбол']: 0.0074\n['Политика', 'Законодательство']: 0.0233\n['Спорт', 'Хоккей']: 0.0112\n['Происшествия', 'Авиакатастрофы']: 0.0118\n['Экономика', 'Энергетика']: 0.0330\n['Происшествия', 'Теракты']: 0.0761\n['Международные отношения', 'Безопасность']: 0.1114\n['Международные отношения', 'Терроризм']: 0.0315\n['Экономика', 'Законодательство']: 0.0073\n['Общество', 'Акции протеста']: 0.0249\n['Происшествия', 'ЧП']: 0.0440\n['Региональные новости', 'Происшествия']: 0.0222\n['Общество', 'Здравоохранение']: 0.0156\n['Спорт', 'Лыжные гонки']: 0.0134\n['Международные отношения', 'Культура']: 0.0144\n['Культура', 'Кино']: 0.0129\n['Происшествия', 'ДТП']: 0.0283\n['Происшествия', 'Убийства']: 0.0136\n['Международные отношения', 'Туризм']: 0.0153\n['Международные отношения', 'Преступность']: 0.0197\n['Экология', 'Катастрофы']: 0.0159\n['Религия', 'Христианство']: 0.0179\n['Происшествия', 'Пожары']: 0.0214\n['Международные отношения', 'Экономика']: 0.0138\n\nВектор CLS токена:\ntensor([[ 4.2627e-01,  1.2238e+00,  1.3749e-01, -1.9489e-01, -4.5104e-01,\n          5.1236e-01, -9.5384e-01,  2.4621e-02, -4.6810e-01, -3.8361e-01,\n         -1.0569e-01,  2.9154e-01, -1.4974e-01,  2.5035e-02,  3.2849e-02,\n          1.6447e-01, -4.0767e-01,  4.8450e-01,  5.2084e-01, -1.6060e-01,\n          1.0766e-01,  8.9824e-02, -4.0986e-01,  6.0340e-01,  4.8670e-01,\n          3.0930e-01,  8.2413e-01, -3.7628e-01, -1.4994e-01,  7.8092e-01,\n          5.5052e-01, -1.8913e-02,  8.2709e-01,  6.8410e-02,  5.8380e-01,\n          1.0960e+00, -4.0943e-02,  1.9500e-01, -1.4211e+00,  7.2741e-02,\n          3.7827e-01, -2.0501e+00,  8.3544e-01,  2.2611e-01,  8.4051e-01,\n         -6.7789e-01, -2.8597e-01, -1.3194e-02,  6.4084e-01,  2.5588e-01,\n         -1.0614e+00,  5.1853e-01,  3.5916e-01, -7.8059e-01, -1.0609e+00,\n         -3.7663e-01, -6.3762e-01, -1.5663e+00,  9.6158e-01, -5.1750e-01,\n         -6.5802e-02, -9.1763e-01, -3.2099e-01,  1.5223e-01,  1.2137e+00,\n         -1.2996e-01,  3.4612e-01, -1.1072e+00,  1.2866e+00,  3.8597e-02,\n         -1.0197e+00, -4.2836e-01, -4.0728e-01, -1.1929e+00, -5.3202e-03,\n         -4.1514e-01, -7.4714e-01, -4.0327e+00, -1.4422e+00, -7.7511e-01,\n         -6.8938e-01,  8.2986e-01,  1.4977e+00, -5.5277e-01, -7.9759e-01,\n         -1.5080e+00,  7.6644e-01, -3.2368e-01,  9.2561e-01,  8.9348e-01,\n          5.7237e-01,  5.0897e-01,  6.6228e-01,  4.9225e-01,  8.4269e-01,\n          9.7481e-02, -5.9250e-01, -4.5540e-01, -5.7619e-01, -1.8531e+00,\n         -8.9069e-01, -8.0590e-01,  8.3651e-01, -5.3148e-01,  2.4010e-01,\n          1.2024e+00, -2.1111e-01, -1.7579e-01, -7.8841e-01,  3.5831e-01,\n         -1.3263e+00, -7.5195e-01,  1.2681e+00,  1.7764e-01, -1.3613e+00,\n          7.8010e-01,  4.4891e-01,  7.1963e-02,  3.7515e-01,  6.4937e-02,\n         -2.1864e-01,  6.7139e-01,  3.6509e-01,  4.9530e-01, -1.3012e+00,\n         -9.0867e-01, -5.8260e-01, -3.6521e-01, -6.5104e-01, -2.3977e-01,\n         -2.4046e-02,  1.2893e-01,  4.6262e-01, -1.0142e+00, -4.1513e-01,\n         -4.5396e-02, -4.6861e-01,  1.8957e-01,  5.5497e-01,  4.6464e-01,\n         -1.1237e+00, -8.7234e-01, -9.8628e-02, -1.5864e-01,  2.7996e-02,\n          2.2230e-01, -3.5224e-02,  8.5603e-01, -6.5743e-01, -2.3400e-01,\n          2.2630e-01, -7.4681e-01, -2.9395e-01,  6.4864e-01,  5.3900e-01,\n         -5.5950e-01, -1.1228e+00,  8.2971e-01, -6.6153e-01, -1.4170e-01,\n          3.0304e-02, -1.2831e-01,  1.9704e-01,  7.8046e-01,  2.9523e-03,\n          4.0669e-01, -2.9722e-01,  3.3929e-01,  6.3632e-01, -3.0203e-02,\n         -4.6577e-01,  5.3113e-01, -8.3228e-02, -7.2491e-01, -7.2842e-01,\n         -4.5655e-01, -1.2545e+00, -6.8278e-01, -6.6222e-01, -8.8258e-01,\n         -3.0870e-01,  1.5036e-01,  6.0172e-02,  1.4823e-02,  5.5596e-01,\n         -6.0360e-01,  2.1496e-02, -1.2463e-01,  4.7559e-01,  5.5469e-01,\n          7.4233e-01, -9.4207e-01,  5.6271e-01,  8.6302e-01, -4.8807e-02,\n         -6.5054e-02,  7.9509e-01,  2.3929e-01, -1.6309e-01, -1.3169e+00,\n          1.4651e+00,  1.7503e+00,  6.2879e-01, -7.4522e-01,  1.3829e+00,\n          1.3203e-01,  2.0929e-01,  2.7612e-01,  1.1232e-01, -5.3232e-01,\n         -2.4151e-01,  2.1864e+00, -1.8081e-01, -1.4445e+00,  6.9835e-01,\n          3.1954e-01, -4.4660e-01,  6.1311e-01,  1.4015e+00,  1.0982e+00,\n         -5.1834e-01, -1.8179e-01,  1.0418e+00, -6.6328e-01,  1.7629e+00,\n         -8.0718e-01,  1.9065e-01,  6.8227e-02,  2.3397e-01, -7.5069e-01,\n         -1.9757e-01, -7.6490e-01, -7.7419e-01,  1.9510e-02,  9.9057e-01,\n         -6.4664e-01, -2.7927e-01,  1.0795e-01, -3.0009e-01, -3.6938e-02,\n         -1.5852e+00,  5.6639e-01,  6.3579e-01,  1.6420e-01,  1.2980e+00,\n          5.1216e-01,  9.5408e-01,  3.3451e+00,  8.8240e-01,  2.0837e-01,\n         -6.5391e-01,  6.3892e-01, -9.4241e-01, -8.5293e-01,  1.5713e+00,\n          1.2510e+00,  8.5513e-01, -2.2345e-01, -1.0356e+00,  8.4320e-01,\n          1.1439e+00, -6.5210e-02,  4.7352e-01, -9.2156e-02,  2.0299e-01,\n         -7.1690e-01, -8.5473e-02,  5.4697e-01,  1.0262e-01,  1.8482e-01,\n         -5.6116e-01, -2.5124e-01, -1.6403e-01, -3.6208e-01,  1.8828e-01,\n          5.8487e-01,  6.2894e-01, -9.1932e-01,  1.0814e+00,  5.3152e-01,\n          5.8042e-01, -5.8980e-01, -8.0568e-01,  1.6524e+00,  8.5807e-01,\n          1.0893e+00, -5.3827e-01, -1.5599e+00,  3.5695e-01,  1.4538e-01,\n          6.9739e-02, -1.0074e+00, -2.2679e-01,  1.8765e-01, -7.2785e-01,\n         -3.3539e-01,  6.6974e-02,  1.5396e+00,  1.2326e-01, -1.3990e-01,\n         -2.9667e-01, -1.6347e+00,  2.9060e-01, -2.4141e-01, -9.9606e-01,\n         -3.5658e-01,  8.0351e-01, -5.3401e-01,  1.0514e-01, -2.2808e-01,\n         -1.0511e+00, -5.5751e-01, -7.1481e-01, -1.8596e-01,  4.8785e-01,\n         -5.6423e-01,  1.0763e-01, -1.6593e+00, -1.0071e+00, -2.5561e-01,\n          6.4279e-01,  1.3567e-01, -5.2808e-01,  1.3835e-01, -1.5380e+00,\n          3.3155e-02, -4.4218e-01, -7.8263e-02, -3.8928e-01,  1.6806e-01,\n         -7.5420e-01,  2.5649e+00,  8.2992e-01, -3.6161e-01,  5.6375e-01,\n          6.9223e-01,  1.0217e-01, -7.5899e-01,  7.4319e-01,  1.0040e+00,\n         -6.6942e-01, -3.8745e-01, -1.0160e+00,  1.7548e-01,  6.4279e-02,\n          4.2685e-01,  2.7909e-01,  4.2574e-02,  1.4035e-01,  1.3623e+00,\n          3.3392e-03, -1.5442e-01,  4.6746e-02, -5.1239e-01,  7.6673e-02,\n         -5.5668e-01, -3.4592e-01, -1.6334e-01,  4.8443e-01, -9.9742e-01,\n          1.9791e-01,  1.9949e+00,  6.7331e-01, -1.6156e-01, -1.2003e+00,\n         -4.8122e-01, -7.4819e-01,  1.4759e+00,  4.4436e-02,  5.7139e-01,\n          3.0867e-01, -3.4231e-02, -2.5678e-01, -4.6190e-01,  3.4399e-02,\n          7.5924e-02, -4.7854e-01, -6.7381e-01, -1.7056e+00,  1.9392e-01,\n          7.3715e-01, -1.3506e+00,  7.5183e-01,  3.1135e-01,  1.9073e+00,\n         -5.8830e-01,  1.6268e-02, -2.4777e-01, -4.6581e-01, -3.2212e-01,\n          8.7901e-02,  7.6911e-01,  5.3919e-01, -5.4771e-01,  2.6054e-01,\n          1.7066e-01,  1.2767e+00,  2.3843e+00,  1.6459e-01,  1.3067e-01,\n         -2.0133e-01,  3.6008e-01, -4.1440e-02, -6.8703e-01,  2.3912e-01,\n         -3.3785e-01, -6.7532e-02, -1.2549e+00, -2.8483e-01, -1.8683e+00,\n         -3.0356e-01, -6.8736e-01,  2.8537e-01, -2.3371e-02,  4.1822e-02,\n         -7.6014e-01,  3.5059e-01, -9.8431e-01, -3.1073e-01, -8.4925e-01,\n         -2.2065e-01,  4.5180e-01,  8.0359e-03,  5.5214e-01, -3.0696e-01,\n          1.1029e-01, -2.8198e-02, -9.0878e-01, -6.9012e-02, -2.4732e-02,\n         -4.0086e-01,  5.2093e-01,  2.1855e-01, -9.8470e-02,  5.1609e-02,\n         -3.9002e-01, -2.0574e-01, -6.1372e-01,  1.1821e-01, -2.3284e-01,\n          7.6210e-01,  7.5978e-01, -2.4659e-01, -8.7123e-01,  3.0442e-01,\n          5.6064e-01, -1.7290e-02,  1.9039e-01,  4.1176e-02, -9.1088e-01,\n         -1.5393e-01, -1.6023e+00, -1.3047e+00, -2.4007e+00,  1.3275e+00,\n         -3.9709e-01, -8.6453e-02,  9.7953e-01,  5.3891e-01,  4.8360e-01,\n         -7.2709e-01, -1.0107e-01, -3.1299e-01, -4.1716e-01,  2.8391e-01,\n         -5.2501e-01, -1.2988e+00,  5.4834e-02,  1.6482e-01,  7.0329e-01,\n          1.0614e+00,  2.5777e-01,  4.8189e-01, -4.8564e-02,  2.4962e-01,\n          5.2887e-01, -1.3584e+00,  3.7959e-01, -1.3803e+00, -7.2461e-01,\n         -6.3602e-01,  5.2719e-01,  5.7284e-01,  8.6349e-01, -8.2521e-01,\n          4.2086e-01, -7.0433e-01,  5.6036e-01,  1.5262e+00,  4.8416e-01,\n         -4.8948e-01, -1.0212e+00, -3.7822e-01,  7.2473e-01, -3.1697e-01,\n         -3.8040e-01, -2.0900e+00,  1.8052e-01,  1.6819e+00,  2.9220e-02,\n          7.3529e-01, -4.0018e-01,  7.8737e-01, -5.1495e-01,  1.4933e-01,\n         -1.4735e+00,  2.6679e-01, -7.7261e-01,  6.7331e-01,  8.0491e-02,\n          7.7092e-01,  1.1678e+00, -1.5816e-01,  7.9906e-01, -2.8022e-01,\n          6.3606e-02,  1.1130e+00, -2.4064e-01, -3.1995e-02,  1.1084e+00,\n         -2.3285e-01, -7.3481e-01, -5.0808e-01,  3.3108e-01, -5.9253e-01,\n         -2.2831e-01,  1.9668e-01, -1.8653e-01,  3.7013e-01, -5.2994e-01,\n          2.9897e-01, -7.6216e-02,  6.3353e-01,  3.0403e-01,  2.6190e-01,\n          8.2616e-01,  8.0632e-01,  1.4471e-01, -4.1269e-01,  1.1431e+00,\n         -2.7193e-01,  6.6452e-01, -5.1661e-01,  2.3770e-01,  7.1143e-01,\n         -1.9302e-01,  4.2050e-03,  6.9121e-01,  8.8830e-01,  7.7884e-02,\n          1.0725e+00, -7.7409e-01, -3.0565e-01,  3.7052e-01, -9.8870e-01,\n          3.9264e-01,  1.9416e-01,  5.9995e-01,  6.3883e-01,  3.6473e-01,\n         -1.0299e+00,  2.6264e-01,  7.8038e-01,  3.6530e-01,  4.3224e-01,\n         -6.7882e-02,  5.6726e-03,  1.0172e+00, -1.3256e-02,  8.7440e-01,\n          4.4487e-01,  1.4276e+00,  2.7496e-01, -1.3580e+00, -2.8037e-02,\n         -3.9238e-01, -5.4984e-01,  2.0821e-01,  2.4369e-02,  7.4704e-01,\n         -8.2573e-01, -3.7643e-01,  1.8884e-01, -2.1465e-01,  1.1526e-02,\n          1.1124e+00,  1.3595e+00, -1.0534e+00,  5.3686e+00,  2.9374e-01,\n          5.9388e-01,  2.6118e-01,  9.9508e-01, -7.3358e-02, -1.5398e-01,\n         -6.4467e-01,  2.8828e-01, -1.5779e-01,  4.5207e-01,  6.4872e-01,\n          9.3315e-01,  3.4412e-01,  2.3403e-01,  4.0388e-01,  9.1241e-01,\n          1.4348e+00,  8.3065e-01, -2.5606e-01, -8.3924e-01,  2.6906e-02,\n         -3.5180e-02,  1.9276e+00,  3.2014e-01, -5.8906e-02,  1.2813e-01,\n          7.4790e-01, -5.3469e-01, -1.3253e+00, -3.7016e-01,  3.1615e-01,\n          7.6666e-01,  1.8066e-01,  1.0888e-01, -9.1925e-01,  8.5779e-01,\n          9.9529e-01,  9.0104e-01,  9.5553e-01, -2.4303e-01, -3.4278e-01,\n          2.8860e-01, -2.7738e-01,  2.1622e-01, -5.3696e-01,  2.5194e-01,\n          1.9653e-01,  9.7888e-01, -8.1721e-02,  1.3860e-01, -4.0680e-01,\n          1.0392e+00,  8.7198e-01, -4.6645e-01, -5.2012e-01,  4.1372e-01,\n         -9.8288e-02,  6.7417e-01,  5.2051e-01,  2.9939e-01,  9.0472e-01,\n          3.1146e-01,  3.0676e-01, -3.7327e-01,  9.0175e-01,  3.9014e-01,\n         -1.9152e+00,  3.4462e-01,  6.4476e-01,  1.3388e+00, -2.2359e-02,\n         -5.0759e-01,  8.8615e-01,  1.9495e+00, -1.0749e-01,  1.0590e+00,\n         -5.9409e-01, -3.9566e-01, -3.0306e-01, -3.3179e-01, -5.4938e-01,\n          9.2773e-01, -2.8393e-01, -5.3346e-01,  5.8026e-01,  2.4825e-01,\n          1.3807e+00, -1.2993e+00, -3.2954e-01,  6.8419e-02,  6.9160e-01,\n         -1.1366e-01, -9.6291e-01,  1.8651e+00, -1.8960e-01, -4.3641e-01,\n         -4.6476e-01,  4.1535e-01,  6.3273e-01, -1.5495e+00,  3.0654e-01,\n          1.3881e-02, -6.9878e-01,  5.9973e-01,  8.1676e-01, -2.9582e-01,\n          5.5243e-01,  6.4913e-01, -5.7377e-01, -2.3002e-02, -2.6534e-01,\n          1.2728e+00, -5.2601e-01, -3.1651e-01, -1.9108e-01,  5.2284e-01,\n          3.3387e-01,  5.4378e-01,  4.6490e-01, -4.4672e-01, -1.1092e+00,\n         -4.3881e-01, -9.4079e-01, -5.2706e-01,  3.9955e-01,  2.6750e-01,\n         -4.5289e-01, -6.4386e-02,  3.5000e-02, -5.2411e-01,  3.5834e-01,\n         -1.0503e-01,  3.8700e-01,  7.1824e-02,  5.4914e-02, -3.7087e-01,\n          6.3726e-02,  1.6789e+00,  4.9158e-01,  6.4606e-01, -5.5239e-01,\n          1.6018e-01, -6.7287e-01, -1.0725e+00, -2.0185e-01,  4.4268e-01,\n         -7.3154e-01,  4.2484e-01, -1.3395e+00, -1.3070e-02,  7.3323e-01,\n         -1.5688e+00, -7.9149e-01,  6.0532e-01,  2.2968e-01,  1.3237e+00,\n          8.0610e-01,  2.3678e-01, -5.1153e-01, -2.0713e-02,  1.6550e+00,\n          1.6396e+00, -6.3390e-02,  7.8823e-01, -5.6050e-01, -2.6965e-02,\n         -7.9437e-01, -9.5305e-01, -2.2556e-01, -8.8257e-02, -4.5381e-01,\n         -6.9640e-01, -1.6932e-01,  1.2519e+00, -2.7110e-01,  1.9689e+00,\n         -1.3425e+00,  5.8045e-01,  5.0594e-01]])\n","output_type":"stream"}],"execution_count":22}]}