# ikanam_news

**Этот проект представляет собой Telegram-бота, который классифицирует новостные сообщения по различным категориям (например, #происшествия, #общество, #культура) с использованием моделей глубокого обучения. Бот анализирует текст входящих сообщений и автоматически присваивает им соответствующие теги.**

## Цель проекта

Основная цель проекта — разработка и внедрение системы автоматической классификации текстовых новостных сообщений для Telegram-бота. Это позволяет:

*   Эффективно организовывать и фильтровать новостной поток.
*   Упростить пользователям поиск интересующей информации по категориям.
*   Автоматизировать процесс тегирования новостей.

## Структура проекта
(некоторые папки сформируются, либо их надо сздать в ходе запуска кода, в гитхабе их нет)

```
ikanam_news/
├── assets/                       # Статические файлы
│   └── shap_plots/
├── data/                         # Данные проекта
│   ├── external/                 # Внешние данные
│   │   └── data_for_llm/
│   ├── raw/                      # Необработанные ("сырые") данные
│   └── user_data/                # Пользовательские данные
├── notebooks/                    # Jupyter ноутбуки для экспериментов и анализа
├── saved_models/                 # Сохраненные обученные модели
├── scripts/                      # Скрипт для запуска авторизации в TG
├── sessions/                     # Файлы сессий
├── src/                          # Исходный код проекта
│   ├── bot/                      # Код для бота
│   ├── core/                     # Основная логика проекта
│   └── workers/                  # Код для воркеров или фоновых задач
├── .gitattributes                # Атрибуты Git
├── .gitignore                    # Файлы и папки, игнорируемые Git
└── environment.yml               # Файл для управления зависимостями Python (Conda)
```

## Установка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/maxlyara1/ikanam_news.git
    cd ikanam_news
    ```

2.  **Создайте и активируйте окружение Conda:**
    Если у вас установлен Conda, вы можете создать окружение на основе файла `environment.yml`:
    ```bash
    conda env create -f environment.yml
    conda activate ikanam_news_env
    ```

    **Установите зависимости (альтернативный способ, если вы используете pip):**
    Если вы используете `pip` и у вас есть файл `requirements.txt` (необходимо его создать, если его нет), выполните:
    ```bash
    pip install -r requirements.txt
    ```

## Конфигурация

Основные параметры проекта задаются в файле `src/config.py`. Ниже приведено описание каждого параметра:

*   `API_KEYS`: Список API ключей Gemini (можно и один указать, но тогда при прочих равных notebooks/train_dataset_getting.ipynb будет дольше отрабатывать и может получить rate limit)
*   `BOT_TOKEN`: Токен вашего Telegram-бота. Можно получить при регистрации бота у @BotFather.
*   `TARGET_CHANNEL_ID`: ID целевого Telegram-канала или чата, куда бот может отправлять сообщения или где он может выполнять какие-либо действия.
*   `TELEGRAM_API_ID`: Ваш API ID для работы с Telegram Client API (библиотека Telethon). Нужен для более продвинутого взаимодействия с Telegram, например, для чтения сообщений из других каналов. В данном случае из новостного канала. Можно получить на сайте https://my.telegram.org/apps
*   `TELEGRAM_API_HASH`: Ваш API Hash для Telegram Client API. Идет в паре с `TELEGRAM_API_ID`.
*   `RIAN_RU_CHANNEL_USERNAME`: Имя пользователя Telegram-канала (например, "@rian_ru"), из которого бот читает новости для последующей классификации.

### Конфигурация SHAP Explainer

Эти параметры управляют работой SHAP (SHapley Additive exPlanations) — метода для интерпретации предсказаний моделей машинного обучения:

*   `SHAP_ENABLED`: Булево значение (`True` или `False`), включающее или отключающее генерацию SHAP-графиков. Эти графики помогают понять, какие слова в тексте новости оказали наибольшее влияние на ее классификацию.
*   `SHAP_TOP_N_ML_EXPLAIN`: Количество мульти-меток, для которых будут генерироваться объяснения SHAP. Например, если `1`, то объяснение будет строиться для самой вероятной мульти-метки.
*   `SHAP_MAX_FEATURES_DISPLAY`: Максимальное количество признаков (слов), отображаемых на waterfall-графике SHAP.
*   `SHAP_NSAMPLES`: Количество сэмплов, используемых SHAP Explainer для аппроксимации значений Шепли. Меньшее значение ускоряет расчеты, но может снизить точность объяснений.

## Используемые модели

Проект использует трансформерную модель **Electra** для классификации текстов. Конкретно, в ноутбуке `notebooks/train_model_process.ipynb` используется предобученная модель для русского языка:

*   **`ai-forever/ruElectra-medium`** (или можно использовать `ai-forever/ruElectra-large` как более мощную альтернативу).

### Архитектура и задачи

Модель дообучается для решения двух задач классификации одновременно (Multi-Task Learning) с использованием кастомной архитектуры `HierarchicalMultiTaskElectra`:

1.  **Многоклассовая классификация (Multi-label Classification):** Каждому новостному сообщению присваивается набор релевантных тегов из предопределенного списка (например, `#происшествия`, `#общество`, `#культура`). Модель предсказывает вероятность принадлежности к каждому тегу.
2.  **Иерархическая классификация (Hierarchical Classification):** В дополнение к основным тегам, модель определяет более специфическую подкатегорию в рамках иерархической структуры. Например, новость с тегом `#Происшествия` может быть уточнена как `['Происшествия', 'Военные действия']` или `['Происшествия', 'Пожары']`.

### Причины выбора Electra

*   **Оптимизация для русского языка:** Модели `ruElectra` от AI Forever (ранее Sber AI) хорошо зарекомендовали себя в задачах обработки естественного русского языка благодаря обучению на больших русскоязычных корпусах.
*   **Эффективность:** Electra предлагает хороший баланс между качеством предсказаний и вычислительной эффективностью (размер модели, скорость инференса) по сравнению с некоторыми другими архитектурами трансформеров. Это делает её подходящей для использования в Telegram-боте, где важна скорость ответа.
*   **Подход Multi-Task Learning:** Реализованная архитектура `HierarchicalMultiTaskElectra` позволяет модели обучаться одновременно на двух связанных задачах. Это способствует лучшему обобщению и извлечению полезных признаков из текста, так как модель учится понимать текст на разных уровнях детализации.
*   **Интерпретируемость с SHAP:** В проект интегрирована возможность использования SHAP для анализа вклада отдельных слов в принятие решения моделью, что повышает прозрачность её работы.

Модель обучается на данных, собранных и обработанных с помощью ноутбука `notebooks/train_dataset_getting.ipynb`, и сохраняется в файл `fine_tuned_electra.pt`.

## Использование

### 1. Получение данных для обучения
Запустите ноутбук `notebooks/train_dataset_getting.ipynb` для сбора и предобработки данных, необходимых для обучения моделей.

### 2. Обучение моделей
Запустите ноутбук `notebooks/train_model_process.ipynb` для обучения моделей классификации текстов.

### 3. Запуск Telegram-бота

*   **Авторизация (однократно):**
    Для первоначальной авторизации вашего Telegram-аккаунта запустите скрипт:
    ```bash
    python -m scripts.authorize_telethon
    ```
    Следуйте инструкциям в консоли. Файл сессии будет сохранен в папку `sessions/`.

*   **Запуск бота:**
    После успешной авторизации запустите основного бота:
    ```bash
    python -m src.bot.telegram_bot
    ```